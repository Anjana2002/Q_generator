{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92316005-0583-49db-a8f1-6c0af92a9f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjana/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:19<00:00,  9.63s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to your PDF file:  /home/anjana/Project/generator/question_papers/note.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting text...\n",
      "\n",
      "\n",
      "Generating questions...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Set up LLM\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    max_new_tokens=1024,  # Increased tokens for more questions\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "def generate_questions(text, num_questions=10):\n",
    "    \"\"\"Generate exam questions directly from text.\"\"\"\n",
    "    prompt = f\"Generate {num_questions} university exam questions from the given content.\"\n",
    "    response = Settings.llm.complete(prompt + \"\\n\\n\" + text)\n",
    "    return response.text\n",
    "\n",
    "def save_questions_to_pdf(questions, output_path=\"Generated_Questions.pdf\"):\n",
    "    \"\"\"Save questions to a PDF file.\"\"\"\n",
    "    doc = fitz.open()\n",
    "    page = doc.new_page()\n",
    "    page.insert_text((50, 50), \"Generated Exam Questions:\\n\\n\" + questions)\n",
    "    doc.save(output_path)\n",
    "    doc.close()\n",
    "    print(f\"Questions saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = input(\"Enter the path to your PDF file: \").strip()\n",
    "\n",
    "    print(\"\\nExtracting text...\\n\")\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"\\nGenerating questions...\\n\")\n",
    "    questions = generate_questions(pdf_text, num_questions=10)  # Generate more questions\n",
    "\n",
    "    print(\"\\nSaving to PDF...\\n\")\n",
    "    save_questions_to_pdf(questions)\n",
    "\n",
    "    print(\"\\nDone! Questions are saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
