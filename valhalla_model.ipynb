{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b2a6fa-5be1-4792-a59e-5ef3b90cf26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjana/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to your PDF file:  /home/anjana/Project/generator/question_papers/note.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Text (Preview):\n",
      " \n",
      "1. Functions\n",
      "1.1\n",
      "Introduction to Functions\n",
      "Every C program must have a main function to indicate where the program has to begin its\n",
      "execution. If a program is written only using a single main function, the program becomes too\n",
      "large and complex and as a result, the task of debugging, testing and maintaining becomes difﬁcult.\n",
      "C functions can be classiﬁed into two categories:\n",
      "• Library Functions: Not required to be writtenby the programmer. e.g. printf(), scanf(),\n",
      "strlen() etc.\n",
      "• User-deﬁned funct ...\n",
      "\n",
      "Split into 6 chunks.\n",
      "\n",
      "Generating Questions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/anjana/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Questions:\n",
      "\n",
      "Chunk 1:\n",
      "- What is the name of the function that is used to describe a program?\n",
      "Chunk 2:\n",
      "- What is the name of the function declaration?\n",
      "Chunk 3:\n",
      "- What is the name of the example for a function with no arguments and no return value?\n",
      "Chunk 4:\n",
      "- What is the name of the variable that is declared inside a function?\n",
      "Chunk 5:\n",
      "- What is the name of the declaration of external variable?\n",
      "Chunk 6:\n",
      "- What is the name of the register variable?\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF extraction\n",
    "from transformers import pipeline\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    \"\"\"Split text into smaller chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i + max_tokens]) for i in range(0, len(words), max_tokens)]\n",
    "    return chunks\n",
    "\n",
    "def generate_questions(text):\n",
    "    \"\"\"Generate questions using a smaller model.\"\"\"\n",
    "    qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-qg-hl\")\n",
    "    questions = qg_pipeline(text)\n",
    "    return questions\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Process a single chunk of text.\"\"\"\n",
    "    return generate_questions(chunk)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = input(\"Enter the path to your PDF file: \").strip()\n",
    "    \n",
    "    # Extract text\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    print(\"\\nExtracted Text (Preview):\\n\", pdf_text[:500], \"...\")  # Show preview\n",
    "\n",
    "    # Chunk text\n",
    "    chunks = chunk_text(pdf_text)\n",
    "    print(f\"\\nSplit into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Generate questions in parallel\n",
    "    print(\"\\nGenerating Questions...\\n\")\n",
    "    with Pool(processes=4) as pool:  # Use 4 processes\n",
    "        results = pool.map(process_chunk, chunks)\n",
    "\n",
    "    # Print generated questions\n",
    "    print(\"\\nGenerated Questions:\\n\")\n",
    "    for i, questions in enumerate(results):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        for q in questions:\n",
    "            print(\"-\", q[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
